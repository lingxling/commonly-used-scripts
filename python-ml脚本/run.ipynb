{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000维，平均值补缺失值\n",
      "sample preprocessed\n",
      "load preprocessed data\n",
      "save train and test data\n",
      "load train and test data\n",
      "--- 开始训练：线性SVM分类器 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9289108187134503\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+-----------+----------+\n",
      "|  class  |    normal   |  missing   |   minor    | outlier |   square   |    trend    |   drift   | accuracy |\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+-----------+----------+\n",
      "|  normal | 2546 98.45% |   0 0.0%   |  40 1.55%  |  0 0.0% |   0 0.0%   |    0 0.0%   |   0 0.0%  |  98.45%  |\n",
      "| missing |    0 0.0%   | 553 94.05% |  34 5.78%  |  0 0.0% |   0 0.0%   |   1 0.17%   |   0 0.0%  |  94.05%  |\n",
      "|  minor  |   25 6.78%  |   0 0.0%   | 344 93.22% |  0 0.0% |   0 0.0%   |    0 0.0%   |   0 0.0%  |  93.22%  |\n",
      "| outlier |  55 50.93%  |   0 0.0%   | 53 49.07%  |  0 0.0% |   0 0.0%   |    0 0.0%   |   0 0.0%  |  0.00%   |\n",
      "|  square |   27 4.62%  |   0 0.0%   |  4 0.68%   |  0 0.0% | 554 94.70% |    0 0.0%   |   0 0.0%  |  94.70%  |\n",
      "|  trend  |    0 0.0%   |  18 1.65%  |   0 0.0%   |  0 0.0% |   0 0.0%   | 1070 97.99% |  4 0.37%  |  97.99%  |\n",
      "|  drift  |    0 0.0%   | 36 25.00%  |   0 0.0%   |  0 0.0% |   0 0.0%   |  92 63.89%  | 16 11.11% |  11.11%  |\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+-----------+----------+\n",
      "load train and test data\n",
      "--- 高斯朴素贝叶斯 ---\n",
      "0.4369517543859649\n",
      "+---------+------------+-----------+------------+-------------+------------+------------+------------+----------+\n",
      "|  class  |   normal   |  missing  |   minor    |   outlier   |   square   |   trend    |   drift    | accuracy |\n",
      "+---------+------------+-----------+------------+-------------+------------+------------+------------+----------+\n",
      "|  normal | 441 17.05% |   0 0.0%  | 795 30.74% | 1350 52.20% |   0 0.0%   |   0 0.0%   |   0 0.0%   |  17.05%  |\n",
      "| missing |   0 0.0%   | 64 10.88% |  34 5.78%  |    0 0.0%   |  3 0.51%   |   0 0.0%   | 487 82.82% |  10.88%  |\n",
      "|  minor  |   0 0.0%   |   0 0.0%  | 358 97.02% |   11 2.98%  |   0 0.0%   |   0 0.0%   |   0 0.0%   |  97.02%  |\n",
      "| outlier |   0 0.0%   |   0 0.0%  | 103 95.37% |   5 4.63%   |   0 0.0%   |   0 0.0%   |   0 0.0%   |  4.63%   |\n",
      "|  square |   0 0.0%   |   0 0.0%  |  24 4.10%  |   2 0.34%   | 559 95.56% |   0 0.0%   |   0 0.0%   |  95.56%  |\n",
      "|  trend  |  25 2.29%  |  15 1.37% |   0 0.0%   |   8 0.73%   |   0 0.0%   | 830 76.01% | 214 19.60% |  76.01%  |\n",
      "|  drift  |  3 2.08%   |   0 0.0%  |  1 0.69%   |   6 4.17%   |   0 0.0%   |   0 0.0%   | 134 93.06% |  93.06%  |\n",
      "+---------+------------+-----------+------------+-------------+------------+------------+------------+----------+\n",
      "load train and test data\n",
      "--- 开始训练：伯努利朴素贝叶斯 ---\n",
      "0.3205409356725146\n",
      "+---------+--------+------------+--------+-------------+--------+--------------+--------+----------+\n",
      "|  class  | normal |  missing   | minor  |   outlier   | square |    trend     | drift  | accuracy |\n",
      "+---------+--------+------------+--------+-------------+--------+--------------+--------+----------+\n",
      "|  normal | 0 0.0% |   0 0.0%   | 0 0.0% | 1506 58.24% | 0 0.0% | 1080 41.76%  | 0 0.0% |  0.00%   |\n",
      "| missing | 0 0.0% | 575 97.79% | 0 0.0% |   2 0.34%   | 0 0.0% |   11 1.87%   | 0 0.0% |  97.79%  |\n",
      "|  minor  | 0 0.0% |   0 0.0%   | 0 0.0% |  112 30.35% | 0 0.0% |  257 69.65%  | 0 0.0% |  0.00%   |\n",
      "| outlier | 0 0.0% |   0 0.0%   | 0 0.0% |  87 80.56%  | 0 0.0% |  21 19.44%   | 0 0.0% |  80.56%  |\n",
      "|  square | 0 0.0% |   0 0.0%   | 0 0.0% |  59 10.09%  | 0 0.0% |  526 89.91%  | 0 0.0% |  0.00%   |\n",
      "|  trend  | 0 0.0% |   0 0.0%   | 0 0.0% |    0 0.0%   | 0 0.0% | 1092 100.00% | 0 0.0% | 100.00%  |\n",
      "|  drift  | 0 0.0% |   0 0.0%   | 0 0.0% |    0 0.0%   | 0 0.0% | 144 100.00%  | 0 0.0% |  0.00%   |\n",
      "+---------+--------+------------+--------+-------------+--------+--------------+--------+----------+\n",
      "load train and test data\n",
      "--- 开始训练：随机森林分类器 ---\n",
      "0.9650950292397661\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+------------+----------+\n",
      "|  class  |    normal   |  missing   |   minor    | outlier |   square   |    trend    |   drift    | accuracy |\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+------------+----------+\n",
      "|  normal | 2577 99.65% |   0 0.0%   |  2 0.08%   | 7 0.27% |   0 0.0%   |    0 0.0%   |   0 0.0%   |  99.65%  |\n",
      "| missing |    0 0.0%   | 586 99.66% |   0 0.0%   |  0 0.0% |  2 0.34%   |    0 0.0%   |   0 0.0%   |  99.66%  |\n",
      "|  minor  |  39 10.57%  |   0 0.0%   | 330 89.43% |  0 0.0% |   0 0.0%   |    0 0.0%   |   0 0.0%   |  89.43%  |\n",
      "| outlier |  51 47.22%  |   0 0.0%   | 48 44.44%  | 9 8.33% |   0 0.0%   |    0 0.0%   |   0 0.0%   |  8.33%   |\n",
      "|  square |   9 1.54%   |   0 0.0%   |   0 0.0%   |  0 0.0% | 576 98.46% |    0 0.0%   |   0 0.0%   |  98.46%  |\n",
      "|  trend  |    0 0.0%   |   0 0.0%   |   0 0.0%   |  0 0.0% |   0 0.0%   | 1076 98.53% |  16 1.47%  |  98.53%  |\n",
      "|  drift  |    0 0.0%   |   0 0.0%   |   0 0.0%   |  0 0.0% |   0 0.0%   |  17 11.81%  | 127 88.19% |  88.19%  |\n",
      "+---------+-------------+------------+------------+---------+------------+-------------+------------+----------+\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "线性SVM分类器, 高斯朴素贝叶斯, 伯努利朴素贝叶斯\n",
    "\"\"\"\n",
    "import utils\n",
    "import linear_svc\n",
    "import gaussian_nb\n",
    "import bernouli_nb\n",
    "import random_forest\n",
    "import show_results\n",
    "from sklearn import metrics\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "print('1000维，平均值补缺失值')\n",
    "\n",
    "utils.sample_preprocessed(step=144)\n",
    "samples, labels = utils.load_preprocessed_data()\n",
    "utils.save_train_and_test_data(samples, labels)\n",
    "\n",
    "svc_res = linear_svc.main()\n",
    "show_results.main(svc_res)\n",
    "\n",
    "gau_res = gaussian_nb.main()\n",
    "show_results.main(gau_res)\n",
    "\n",
    "ber_res = bernouli_nb.main()\n",
    "show_results.main(ber_res)\n",
    "\n",
    "rf_res = random_forest.main()\n",
    "show_results.main(rf_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train and test data\n",
      "WARNING:tensorflow:From c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ts (InputLayer)                 (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fs (InputLayer)                 (None, 600)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 600, 1)       0           ts[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 600, 1)       0           fs[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 594, 64)      512         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 594, 64)      512         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 594, 64)      256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 594, 64)      256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 198, 64)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 198, 64)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 192, 128)     57472       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 192, 128)     57472       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 192, 128)     512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 192, 128)     512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 64, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 64, 128)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16384)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2097280     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            903         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,215,687\n",
      "Trainable params: 2,214,919\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(21888, 1200) (21888, 7) (5472, 1200) (5472, 7)\n",
      "WARNING:tensorflow:From c:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 21888 samples, validate on 5472 samples\n",
      "Epoch 1/20\n",
      "21888/21888 [==============================] - 176s 8ms/step - loss: 0.2888 - acc: 0.9201 - val_loss: 0.1834 - val_acc: 0.9512\n",
      "Epoch 2/20\n",
      "21888/21888 [==============================] - 175s 8ms/step - loss: 0.1921 - acc: 0.9465 - val_loss: 0.1837 - val_acc: 0.9496\n",
      "Epoch 3/20\n",
      "21888/21888 [==============================] - 175s 8ms/step - loss: 0.1807 - acc: 0.9513 - val_loss: 0.1764 - val_acc: 0.9519\n",
      "Epoch 4/20\n",
      "21888/21888 [==============================] - 178s 8ms/step - loss: 0.3107 - acc: 0.9401 - val_loss: 0.1771 - val_acc: 0.9527\n",
      "Epoch 5/20\n",
      "21888/21888 [==============================] - 174s 8ms/step - loss: 0.1802 - acc: 0.9526 - val_loss: 0.1694 - val_acc: 0.9532\n",
      "Epoch 6/20\n",
      "21888/21888 [==============================] - 176s 8ms/step - loss: 0.1734 - acc: 0.9536 - val_loss: 0.1722 - val_acc: 0.9550\n",
      "Epoch 7/20\n",
      "21888/21888 [==============================] - 186s 8ms/step - loss: 0.1717 - acc: 0.9535 - val_loss: 0.1727 - val_acc: 0.9530\n",
      "Epoch 8/20\n",
      "21888/21888 [==============================] - 190s 9ms/step - loss: 0.1703 - acc: 0.9533 - val_loss: 0.1650 - val_acc: 0.9554\n",
      "Epoch 9/20\n",
      "21888/21888 [==============================] - 201s 9ms/step - loss: 0.1666 - acc: 0.9537 - val_loss: 0.1562 - val_acc: 0.9543\n",
      "Epoch 10/20\n",
      "21888/21888 [==============================] - 187s 9ms/step - loss: 0.2254 - acc: 0.9499 - val_loss: 1.8076 - val_acc: 0.8520\n",
      "Epoch 11/20\n",
      "21888/21888 [==============================] - 183s 8ms/step - loss: 1.8060 - acc: 0.8511 - val_loss: 1.8079 - val_acc: 0.8509\n",
      "Epoch 12/20\n",
      "21888/21888 [==============================] - 173s 8ms/step - loss: 1.7992 - acc: 0.8543 - val_loss: 1.8101 - val_acc: 0.8523\n",
      "Epoch 13/20\n",
      "21888/21888 [==============================] - 178s 8ms/step - loss: 1.7979 - acc: 0.8532 - val_loss: 1.7954 - val_acc: 0.8551\n",
      "Epoch 14/20\n",
      "21888/21888 [==============================] - 173s 8ms/step - loss: 1.7921 - acc: 0.8548 - val_loss: 1.7882 - val_acc: 0.8564\n",
      "Epoch 15/20\n",
      "21888/21888 [==============================] - 174s 8ms/step - loss: 1.7895 - acc: 0.8559 - val_loss: 1.7804 - val_acc: 0.8600\n",
      "Epoch 16/20\n",
      "21888/21888 [==============================] - 175s 8ms/step - loss: 1.7834 - acc: 0.8576 - val_loss: 1.7867 - val_acc: 0.8589\n",
      "Epoch 17/20\n",
      "21888/21888 [==============================] - 173s 8ms/step - loss: 1.7760 - acc: 0.8591 - val_loss: 1.7725 - val_acc: 0.8613\n",
      "Epoch 18/20\n",
      "21888/21888 [==============================] - 181s 8ms/step - loss: 1.7755 - acc: 0.8579 - val_loss: 1.7734 - val_acc: 0.8622\n",
      "Epoch 19/20\n",
      "21888/21888 [==============================] - 183s 8ms/step - loss: 1.7665 - acc: 0.8615 - val_loss: 1.8310 - val_acc: 0.8403\n",
      "Epoch 20/20\n",
      "21888/21888 [==============================] - 186s 8ms/step - loss: 1.7711 - acc: 0.8610 - val_loss: 1.7734 - val_acc: 0.8602\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-5.31581000e-01, -5.31581000e-01, -5.31581000e-01, ...,\n         1.27019224e+00,  1.20325644e+00,  1.16244148e+00],\n       [ 1.56150000e-02,  1.71690000e-02,  1.60240000e-02, ...,\n         2....",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bb0660bdffb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\IPC-SHM\\src\\CNN1d.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X_test, batch_size)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lxl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[-5.31581000e-01, -5.31581000e-01, -5.31581000e-01, ...,\n         1.27019224e+00,  1.20325644e+00,  1.16244148e+00],\n       [ 1.56150000e-02,  1.71690000e-02,  1.60240000e-02, ...,\n         2...."
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "采样+CNN\n",
    "\"\"\"\n",
    "import CNN1d\n",
    "import utils\n",
    "import show_results\n",
    "from sklearn import metrics\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "utils.sample_preprocessed()\n",
    "samples, labels = utils.load_preprocessed_data()\n",
    "utils.save_train_and_test_data(samples, labels)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = utils.load_train_and_test_data()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y_train = encoder.fit_transform(Y_train)\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y_test = encoder.fit_transform(Y_test)\n",
    "Y_test = np_utils.to_categorical(Y_test)\n",
    "\n",
    "cnn_model = CNN1d.CNN1D(input_shape=600, num_classes=7, structure=1)\n",
    "\n",
    "\n",
    "cnn_model.train_model(X_train, Y_train, X_test, Y_test, structure=1)\n",
    "\n",
    "pred=cnn_model.predict(X_test)\n",
    "show_results.main((Y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "将数据进行切片\n",
    "训练CNN1D模型，再训练NN聚合结果\n",
    "\"\"\"\n",
    "\n",
    "import CNN1d\n",
    "import AggregationNN\n",
    "import utils\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "\n",
    "# 前30天的数据用于训练，最后一天的数据用于验证\n",
    "# iter_time=1表示每天的数据只用一次\n",
    "cnn_model = CNN1d.main(iter_time=5, batch_size=64)  # 训练、保存模型\n",
    "\n",
    "cnn_model = CNN1d.CNN1D(input_shape=1200, num_classes=7)\n",
    "cnn_model.load_CNN_model()\n",
    "\n",
    "\n",
    "# 保存中间预测结果\n",
    "if not os.path.exists('../CNN_temp_data/'):\n",
    "    os.mkdir('../CNN_temp_data/')\n",
    "for i in range(0, 29):\n",
    "    utils.delete_data()\n",
    "    begin, end = i, i + 1\n",
    "    utils.simple_preprocessed(begin, end)\n",
    "    X_test, _ = utils.get_slice_data((120, 600), begin, end)  # (109440, 1200)\n",
    "    inter = cnn_model.model.predict(X_test, batch_size=64) \n",
    "    number = inter.shape[0]//120  # 每个样例被分成了120个片段，\n",
    "    inter = np.reshape(inter, (number, 120*7))  # 转成(number, 120, 7)，即N个样例，每个样例的尺寸是120*7\n",
    "    \n",
    "    _, Y_test = utils.load_preprocessed_data(begin, end)  \n",
    "    \n",
    "    # 将Y编码为one-hot\n",
    "    encoder = LabelEncoder()\n",
    "    Y_test = encoder.fit_transform(Y_test)\n",
    "    Y_test = np_utils.to_categorical(Y_test)\n",
    "    \n",
    "    print('inter', inter.shape, 'Y_test', Y_test.shape)\n",
    "    np.save('../CNN_temp_data/' + str(i) + '_intermediate_results', inter)\n",
    "    np.save('../CNN_temp_data/' + str(i) + '_final_results', Y_test)\n",
    "\n",
    "# 训练聚合网络\n",
    "aggrNN = AggregationNN.main(iter_time=5, batch_size=128)\n",
    "\n",
    "# 测试在最后一天的数据上的效果\n",
    "utils.delete_data()\n",
    "utils.simple_preprocessed(29, 30)\n",
    "samples, labels = utils.get_slice_data((120, 600), 29, 30)\n",
    "inter_pred = cnn_model.predict(samples, batch_size=64)  \n",
    "\n",
    "number = inter_pred.shape[0]//120\n",
    "inter_pred = np.reshape(inter_pred, (number, 120*7))\n",
    "final_pred = aggrNN.predict(inter_pred, batch_size=128)\n",
    "\n",
    "_, labels = utils.load_preprocessed_data(29, 30)\n",
    "\n",
    "print(final_pred.shape, labels.shape)\n",
    "\n",
    "final_pred = np.argmax(final_pred, axis=1)\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)\n",
    "\n",
    "acc = metrics.accuracy_score(labels, final_pred)\n",
    "\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "# -*- coding: UTF-8 -*-\n",
     "\n",
     "\"\"\"\n",
     "线性SVM分类器, 高斯朴素贝叶斯, 伯努利朴素贝叶斯\n",
     "\"\"\"\n",
     "import utils\n",
     "import linear_svc\n",
     "import gaussian_nb\n",
     "import bernouli_nb\n",
     "\n",
     "utils.max_preprocessed()\n",
     "samples, labels = utils.load_preprocessed_data()\n",
     "utils.save_train_and_test_data(samples, labels)\n",
     "\n",
     "linear_svc.main()\n",
     "gaussian_nb.main()\n",
     "bernouli_nb.main()\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
